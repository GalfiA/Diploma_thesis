{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Diploma Thesis -- Gálfi András\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task: Named Entity Recognition (_A Natural Language Processing task_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### _Step 1_: Import in all the necesssary libraries\n",
    "\n",
    "   (there can be imports later in this notebook, up here these are the ones usually needed for an NLP task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:60% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:60% !important; }</style>\"))\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%pylab inline\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### _Step 2_: Import the data\n",
    "\n",
    "   We need data because we want to use a neural network for this task and it needs the dataset for learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### _Function_ read_and_sentence:\n",
    "\n",
    "   Divide the given file into two datasets, *words* and *labels*:\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_sentence(file_path):\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    raw_text = file_path.read_text().strip()\n",
    "    raw_sentences = re.split(r'\\n\\t?\\n', raw_text)\n",
    "    \n",
    "    sentence_tokens = []\n",
    "    sentence_tags = []\n",
    "    \n",
    "    for sents in raw_sentences:\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        \n",
    "        for line in sents.split('\\n'):\n",
    "            token = line.split()[0]\n",
    "            tag = line.split()[3]\n",
    "            \n",
    "            tokens.append(token)\n",
    "            # tags.append(entity_to_number[tag])\n",
    "            tags.append(tag)\n",
    "            \n",
    "        sentence_tokens.append(tokens)\n",
    "        sentence_tags.append(tags)\n",
    "    \n",
    "    return sentence_tokens, sentence_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_full, train_tags_full = read_and_sentence('E:\\Egyetem\\Diplomaterv\\data\\conllpp_train.txt')\n",
    "dev_data_full, dev_tags_full = read_and_sentence('E:\\Egyetem\\Diplomaterv\\data\\conllpp_dev.txt')\n",
    "test_data_full, test_tags_full = read_and_sentence('E:\\Egyetem\\Diplomaterv\\data\\conllpp_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['-DOCSTART-'],\n",
       " ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'],\n",
       " ['Peter', 'Blackburn'],\n",
       " ['BRUSSELS', '1996-08-22'],\n",
       " ['The',\n",
       "  'European',\n",
       "  'Commission',\n",
       "  'said',\n",
       "  'on',\n",
       "  'Thursday',\n",
       "  'it',\n",
       "  'disagreed',\n",
       "  'with',\n",
       "  'German',\n",
       "  'advice',\n",
       "  'to',\n",
       "  'consumers',\n",
       "  'to',\n",
       "  'shun',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  'until',\n",
       "  'scientists',\n",
       "  'determine',\n",
       "  'whether',\n",
       "  'mad',\n",
       "  'cow',\n",
       "  'disease',\n",
       "  'can',\n",
       "  'be',\n",
       "  'transmitted',\n",
       "  'to',\n",
       "  'sheep',\n",
       "  '.']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_full[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### _Step 3_: Prepare the data\n",
    "\n",
    "In their current form the datasets can't be used for teaching. We need to prepare the data and make it understandable for the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell I modified the size of the datasets. I used it when I tested the whole project and it was more practical to use smaller datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train_data = train_data_full[:4000]\n",
    "## train_tags = train_tags_full[:4000]\n",
    "\n",
    "## dev_data = dev_data_full[:1000]\n",
    "## dev_tags = dev_tags_full[:1000]\n",
    "\n",
    "## test_data = test_data_full[:1000]\n",
    "## test_tags = test_tags_full[:1000]\n",
    "\n",
    "train_data = train_data_full\n",
    "train_tags = train_tags_full\n",
    "\n",
    "dev_data = dev_data_full\n",
    "dev_tags = dev_tags_full\n",
    "\n",
    "test_data = test_data_full\n",
    "test_tags = test_tags_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Create vocabularies:\n",
    "\n",
    "1. turn the datasets (list of lists) into a single list\n",
    "2. _function_ create_vocab:\n",
    "    1. selecting the unique members of the list\n",
    "    2. giving every unique element a number to represent it\n",
    "3. two vocab is needed:\n",
    "    1. one for the words --> *word_vocab*\n",
    "    2. one for the labels --> *label_vocab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import hashing_trick\n",
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_words = [i for sublist in train_data for i in sublist]\n",
    "dv_words = [i for sublist in dev_data for i in sublist]\n",
    "tst_words = [i for sublist in test_data for i in sublist]\n",
    "\n",
    "tr_tags = [i for sublist in train_tags for i in sublist]\n",
    "dv_tags = [i for sublist in dev_tags for i in sublist]\n",
    "tst_tags = [i for sublist in test_tags for i in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30290"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = set(tr_words + dv_words + tst_words)\n",
    "vocab_size = len(words)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(tr_words + dv_words + tst_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print(t.word_counts)\n",
    "## print(t.document_count)\n",
    "## print(t.word_docs)\n",
    "\n",
    "## word_vocab = t.word_index\n",
    "\n",
    "## print(t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(word_list):\n",
    "    unique_list = []\n",
    "    vocab = {}\n",
    "    \n",
    "    for val in word_list:\n",
    "        if val not in unique_list:\n",
    "            unique_list.append(val)\n",
    "            \n",
    "    for i, l in enumerate(unique_list):\n",
    "        vocab[l] = i\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab = create_vocab(tr_words + dv_words + tst_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vocab = create_vocab(tr_tags+dv_tags+tst_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-ORG': 1,\n",
       " 'B-MISC': 2,\n",
       " 'B-PER': 3,\n",
       " 'I-PER': 4,\n",
       " 'B-LOC': 5,\n",
       " 'I-ORG': 6,\n",
       " 'I-MISC': 7,\n",
       " 'I-LOC': 8}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(label_vocab))\n",
    "label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30290"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### *Function* check_similar_label_length:\n",
    "\n",
    "Check the longest streak of similar labels in the dataset.\n",
    "(I used this function only to get to know the data a little better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similar_label_length(data):\n",
    "    current_label = 'O'\n",
    "    current_length = 0\n",
    "    longest_similar_length = 0\n",
    "    longest_label = 'O'\n",
    "    \n",
    "    for sentence in data:\n",
    "        current_label = 'O'\n",
    "        current_length = 0\n",
    "        \n",
    "        for i in sentence:\n",
    "            if i == 'O':\n",
    "                if current_length > longest_similar_length:\n",
    "                    longest_similar_length = current_length\n",
    "                    longest_label = current_label\n",
    "                    \n",
    "                current_label = 'O'\n",
    "                current_length = 0\n",
    "                \n",
    "            elif i != current_label:\n",
    "                \n",
    "                if current_length > longest_similar_length:\n",
    "                    longest_similar_length = current_length\n",
    "                    longest_label = current_label\n",
    "                \n",
    "                current_label = i\n",
    "                current_length = 1\n",
    "            \n",
    "            else:\n",
    "                current_length += 1\n",
    "    return longest_similar_length, longest_label\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 'I-ORG')\n",
      "(9, 'I-ORG')\n",
      "(5, 'I-MISC')\n"
     ]
    }
   ],
   "source": [
    "print(check_similar_label_length(train_tags))\n",
    "print(check_similar_label_length(dev_tags))\n",
    "print(check_similar_label_length(test_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check_label_order function:\n",
    "\n",
    "This function is checking whether there is an appropiate B-xxx tag in fornt of each I-xxx sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_label_order(data): \n",
    "    for sentence in data:\n",
    "        if 'I-PER' in sentence:\n",
    "            if 'B-PER' not in sentence:\n",
    "                return False, sentence\n",
    "        \n",
    "        if 'I-ORG' in sentence:\n",
    "            if 'B-ORG' not in sentence:\n",
    "                return False, sentence\n",
    "        \n",
    "        if 'I-MISC' in sentence:\n",
    "            if 'B-MISC' not in sentence:\n",
    "                return False, sentence\n",
    "            \n",
    "        if 'I-LOC' in sentence:\n",
    "            if 'B-LOC' not in sentence:\n",
    "                return False, sentence\n",
    "            \n",
    "        \n",
    "        for i, j in enumerate(sentence[:-1]):\n",
    "            if sentence[i + 1] ==  'I-PER':\n",
    "                    if sentence[i] != 'I-PER' and sentence[i] != 'B-PER':\n",
    "                        return False, sentence\n",
    "            if sentence[i + 1] ==  'I-ORG':\n",
    "                    if sentence[i] != 'I-ORG' and sentence[i] != 'B-ORG':\n",
    "                        return False, sentence\n",
    "            if sentence[i + 1] ==  'I-MISC':\n",
    "                    if sentence[i] != 'I-MISC' and sentence[i] != 'B-MISC':\n",
    "                        return False, sentence\n",
    "            if sentence[i + 1] ==  'I-LOC':\n",
    "                    if sentence[i] != 'I-LOC' and sentence[i] != 'B-LOC':\n",
    "                        return False, sentence\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(check_label_order(train_tags))\n",
    "print(check_label_order(dev_tags))\n",
    "print(check_label_order(test_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell makes tuples from the words and the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = list(zip(train_data, train_tags))\n",
    "eval_data = list(zip(dev_data, dev_tags))\n",
    "testing_data = list(zip(test_data, test_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function _prepare_sequence_:\n",
    "Parameters:\n",
    "- seq: a list of elements, this case it is a sentence\n",
    "- to_ix: a vocabulary that contains the elements of the seq\n",
    "\n",
    "The function assigns a number for every element of the sequence (according to the vocab). And returns it in a torch.tensor.long format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 170524,\n",
       "         1: 6321,\n",
       "         2: 3438,\n",
       "         3: 6600,\n",
       "         4: 4528,\n",
       "         5: 7140,\n",
       "         6: 3704,\n",
       "         7: 1155,\n",
       "         8: 1157})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "label_collection = collections.Counter([label_vocab[w] for w in tr_tags])\n",
    "label_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[170524, 6321, 3438, 6600, 4528, 7140, 3704, 1155, 1157]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## sum(list(label_collection.values()))\n",
    "list(label_collection.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(label_collection.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def calc_weights(label_list):\n",
    "    weights = []\n",
    "    label_collection = collections.Counter(label_list)\n",
    "    label_values = list(label_collection.values())\n",
    "    label_sum = sum(label_values)\n",
    "    \n",
    "    weights = label_sum / label_values\n",
    "    \n",
    "    ## for i in range(len(label_collection.values())):\n",
    "        ## weights.append(label_sum / (len(label_values) * label_values[i]))\n",
    "    return weights\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.1996,  32.3631,  59.5017,  30.9950,  45.1782,  28.6508,  55.2287,\n",
       "        177.1143, 176.8081])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.Tensor(calc_weights([label_vocab[w] for w in tr_tags]))\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 4: Neural Network\n",
    "\n",
    "Now that the data is ready the only thing remains is the Neural Network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First there are a a few global parameters that need to be assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word_vocab)\n",
    "OUT_DIM = len(label_vocab)\n",
    "EMBED_DIM = 64\n",
    "HIDDEN_DIM = 32\n",
    "learning_rate = 0.01\n",
    "\n",
    "BATCH_SIZE = 256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the cell, where the model is defined.\n",
    "\n",
    "# (*TODO: more sophisticated model*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model, loss and optimizer are defined here. The model is assigned for the class created in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(EMBED_DIM, HIDDEN_DIM, VOCAB_SIZE, OUT_DIM)\n",
    "model.to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(weight=weights)\n",
    "loss_function.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     inputs = prepare_sequence(training_data[1][0], word_vocab)\n",
    "#     tag_scores = model(inputs)\n",
    "#     print(tag_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function *class_performance*:\n",
    "\n",
    "This function helps evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def class_performance(preds, y):\n",
    "\n",
    "    rounded_preds = preds.argmax(1)\n",
    "\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(\n",
    "        rounded_preds.cpu(), y.cpu()\n",
    "    )\n",
    "\n",
    "    return precision[0], recall[0], fscore[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the part where the training is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, training_data, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_prec = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_fscore = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for sentence, tags in training_data:\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        sentence_in = prepare_sequence(sentence, word_vocab)\n",
    "        targets = prepare_sequence(tags, label_vocab)\n",
    "        \n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        loss = criterion(tag_scores, targets)\n",
    "        prec, recall, fscore = class_performance(tag_scores, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_prec += prec.item()\n",
    "        epoch_recall += recall.item()\n",
    "        epoch_fscore += fscore.item()\n",
    "        \n",
    "    return (epoch_loss / len(training_data),\n",
    "            epoch_prec / len(training_data),\n",
    "            epoch_recall / len(training_data),\n",
    "            epoch_fscore / len(training_data),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the part where the evaluation is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, eval_data, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_prec = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_fscore = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for sentence, tags in eval_data:\n",
    "            \n",
    "            sentence_in = prepare_sequence(sentence, word_vocab)\n",
    "            targets = prepare_sequence(tags, label_vocab)\n",
    "            \n",
    "            tag_scores = model(sentence_in)\n",
    "            loss = criterion(tag_scores, targets)\n",
    "            \n",
    "            prec, recall, fscore = class_performance(tag_scores, targets)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_prec += prec.item()\n",
    "            epoch_recall += recall.item()\n",
    "            epoch_fscore += fscore.item()\n",
    "        \n",
    "    return (\n",
    "        epoch_loss / len(test_data),\n",
    "        epoch_prec / len(test_data),\n",
    "        epoch_recall / len(test_data),\n",
    "        epoch_fscore / len(test_data),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function *epoch_time*:\n",
    "\n",
    "Shows how much time has passed since the last epoch during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(5):\n",
    "#     print(\"Current epoch: \")\n",
    "    \n",
    "    \n",
    "    \n",
    "#     for sentence, tags in training_data:\n",
    "#         model.zero_grad()\n",
    "        \n",
    "#         sentence_in = prepare_sequence(sentence, word_vocab)\n",
    "#         targets = prepare_sequence(tags, label_vocab)\n",
    "\n",
    "#         tag_scores = model(sentence_in)\n",
    "\n",
    "#         loss = loss_function(tag_scores, targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is where the training and the evaluation are actually called. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 3m 56s\n",
      "\tTrain Loss: 1.694 | Train Prec: 84.89% | Train Rec: 81.56% | Train Fscore: 81.29%\n",
      "\t Val. Loss: 1.477 |  Val Prec: 79.96% | Val Rec: 78.52% | Val Fscore: 78.11%\n",
      "Epoch: 02 | Epoch Time: 4m 10s\n",
      "\tTrain Loss: 1.346 | Train Prec: 79.60% | Train Rec: 87.80% | Train Fscore: 82.00%\n",
      "\t Val. Loss: 1.239 |  Val Prec: 73.83% | Val Rec: 82.08% | Val Fscore: 76.55%\n",
      "Epoch: 03 | Epoch Time: 4m 17s\n",
      "\tTrain Loss: 1.122 | Train Prec: 78.99% | Train Rec: 90.56% | Train Fscore: 82.97%\n",
      "\t Val. Loss: 1.107 |  Val Prec: 73.31% | Val Rec: 83.22% | Val Fscore: 76.82%\n",
      "Epoch: 04 | Epoch Time: 4m 16s\n",
      "\tTrain Loss: 0.975 | Train Prec: 79.30% | Train Rec: 91.87% | Train Fscore: 83.78%\n",
      "\t Val. Loss: 1.026 |  Val Prec: 74.16% | Val Rec: 83.85% | Val Fscore: 77.62%\n",
      "Epoch: 05 | Epoch Time: 4m 25s\n",
      "\tTrain Loss: 0.860 | Train Prec: 79.64% | Train Rec: 92.72% | Train Fscore: 84.39%\n",
      "\t Val. Loss: 0.961 |  Val Prec: 74.66% | Val Rec: 84.46% | Val Fscore: 78.25%\n",
      "Epoch: 06 | Epoch Time: 4m 20s\n",
      "\tTrain Loss: 0.763 | Train Prec: 80.06% | Train Rec: 93.48% | Train Fscore: 84.99%\n",
      "\t Val. Loss: 0.910 |  Val Prec: 75.47% | Val Rec: 85.00% | Val Fscore: 78.95%\n",
      "Epoch: 07 | Epoch Time: 4m 15s\n",
      "\tTrain Loss: 0.687 | Train Prec: 80.89% | Train Rec: 94.22% | Train Fscore: 85.85%\n",
      "\t Val. Loss: 0.875 |  Val Prec: 76.26% | Val Rec: 85.67% | Val Fscore: 79.70%\n",
      "Epoch: 08 | Epoch Time: 4m 0s\n",
      "\tTrain Loss: 0.629 | Train Prec: 81.69% | Train Rec: 94.73% | Train Fscore: 86.59%\n",
      "\t Val. Loss: 0.854 |  Val Prec: 76.89% | Val Rec: 86.09% | Val Fscore: 80.29%\n",
      "Epoch: 09 | Epoch Time: 4m 13s\n",
      "\tTrain Loss: 0.581 | Train Prec: 82.38% | Train Rec: 95.13% | Train Fscore: 87.21%\n",
      "\t Val. Loss: 0.842 |  Val Prec: 77.44% | Val Rec: 86.29% | Val Fscore: 80.72%\n",
      "Epoch: 10 | Epoch Time: 4m 8s\n",
      "\tTrain Loss: 0.541 | Train Prec: 83.15% | Train Rec: 95.60% | Train Fscore: 87.90%\n",
      "\t Val. Loss: 0.835 |  Val Prec: 77.61% | Val Rec: 86.32% | Val Fscore: 80.84%\n",
      "Epoch: 11 | Epoch Time: 4m 8s\n",
      "\tTrain Loss: 0.505 | Train Prec: 83.83% | Train Rec: 95.98% | Train Fscore: 88.49%\n",
      "\t Val. Loss: 0.830 |  Val Prec: 77.95% | Val Rec: 86.54% | Val Fscore: 81.14%\n",
      "Epoch: 12 | Epoch Time: 4m 16s\n",
      "\tTrain Loss: 0.474 | Train Prec: 84.46% | Train Rec: 96.30% | Train Fscore: 89.03%\n",
      "\t Val. Loss: 0.827 |  Val Prec: 78.22% | Val Rec: 86.69% | Val Fscore: 81.35%\n",
      "Epoch: 13 | Epoch Time: 4m 23s\n",
      "\tTrain Loss: 0.445 | Train Prec: 85.00% | Train Rec: 96.55% | Train Fscore: 89.48%\n",
      "\t Val. Loss: 0.827 |  Val Prec: 78.42% | Val Rec: 86.80% | Val Fscore: 81.51%\n",
      "Epoch: 14 | Epoch Time: 4m 20s\n",
      "\tTrain Loss: 0.419 | Train Prec: 85.57% | Train Rec: 96.85% | Train Fscore: 89.97%\n",
      "\t Val. Loss: 0.827 |  Val Prec: 78.80% | Val Rec: 86.95% | Val Fscore: 81.81%\n",
      "Epoch: 15 | Epoch Time: 4m 24s\n",
      "\tTrain Loss: 0.395 | Train Prec: 86.04% | Train Rec: 97.11% | Train Fscore: 90.38%\n",
      "\t Val. Loss: 0.826 |  Val Prec: 79.03% | Val Rec: 86.97% | Val Fscore: 81.96%\n",
      "Epoch: 16 | Epoch Time: 3m 37s\n",
      "\tTrain Loss: 0.373 | Train Prec: 86.42% | Train Rec: 97.30% | Train Fscore: 90.70%\n",
      "\t Val. Loss: 0.828 |  Val Prec: 79.25% | Val Rec: 87.12% | Val Fscore: 82.15%\n",
      "Epoch: 17 | Epoch Time: 2m 36s\n",
      "\tTrain Loss: 0.353 | Train Prec: 86.85% | Train Rec: 97.41% | Train Fscore: 91.01%\n",
      "\t Val. Loss: 0.831 |  Val Prec: 79.61% | Val Rec: 87.18% | Val Fscore: 82.40%\n",
      "Epoch: 18 | Epoch Time: 2m 36s\n",
      "\tTrain Loss: 0.334 | Train Prec: 87.23% | Train Rec: 97.52% | Train Fscore: 91.30%\n",
      "\t Val. Loss: 0.834 |  Val Prec: 79.78% | Val Rec: 87.18% | Val Fscore: 82.52%\n",
      "Epoch: 19 | Epoch Time: 2m 53s\n",
      "\tTrain Loss: 0.317 | Train Prec: 87.56% | Train Rec: 97.63% | Train Fscore: 91.56%\n",
      "\t Val. Loss: 0.838 |  Val Prec: 79.98% | Val Rec: 87.27% | Val Fscore: 82.68%\n",
      "Epoch: 20 | Epoch Time: 2m 41s\n",
      "\tTrain Loss: 0.301 | Train Prec: 87.87% | Train Rec: 97.69% | Train Fscore: 91.79%\n",
      "\t Val. Loss: 0.842 |  Val Prec: 80.09% | Val Rec: 87.28% | Val Fscore: 82.75%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_prec, train_rec, train_fscore = train(model, training_data, loss_function)\n",
    "    \n",
    "    valid_loss, valid_prec, valid_rec, valid_fscore = evaluate(model, eval_data, loss_function)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train Prec: {train_prec*100:.2f}% | Train Rec: {train_rec*100:.2f}% | Train Fscore: {train_fscore*100:.2f}%\")\n",
    "    print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val Prec: {valid_prec*100:.2f}% | Val Rec: {valid_rec*100:.2f}% | Val Fscore: {valid_fscore*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['West', 'Indies', 'captain', 'Courtney', 'Walsh', 'elected', 'to', 'bat', 'after', 'winning', 'the', 'toss', 'in', 'the', 'first', 'match', 'in', 'the', 'World', 'Series', 'limited', 'overs', 'competition', 'against', 'Australia', 'at', 'the', 'Melbourne', 'Cricket', 'Ground', 'on', 'Friday', '.']\n",
      "['B-LOC', 'I-LOC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "sent_num = 337\n",
    "print(testing_data[sent_num][0])\n",
    "print(testing_data[sent_num][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1063, 13963,  2753, 13972, 13973,  8730,     5,  2313,   119,  3117,\n",
      "           40, 14772,   236,    40,  1394,  1908,   236,    40,  1787,  1788,\n",
      "         3703,  3704,  1578,   788,  1832,   156,    40,  9889, 12910, 27289,\n",
      "           18,  1162,     9])\n",
      "tensor([1, 8, 6, 6, 4, 4, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 7, 2, 7, 0, 0,\n",
      "        5, 5, 0, 1, 6, 5, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(testing_data[sent_num][0], word_vocab)\n",
    "    tag_scores = model(inputs)\n",
    "    prediction = tag_scores.argmax(1)\n",
    "\n",
    "    print(inputs)\n",
    "    ## print(tag_scores)\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91     35557\n",
      "           1       0.50      0.43      0.46      1992\n",
      "           2       0.57      0.40      0.47      1031\n",
      "           3       0.45      0.43      0.44      1689\n",
      "           4       0.50      0.45      0.48      1287\n",
      "           5       0.74      0.50      0.59      2427\n",
      "           6       0.56      0.27      0.37      1793\n",
      "           7       0.48      0.32      0.38       381\n",
      "           8       0.63      0.32      0.42       509\n",
      "\n",
      "    accuracy                           0.82     46666\n",
      "   macro avg       0.59      0.45      0.50     46666\n",
      "weighted avg       0.80      0.82      0.81     46666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "out_sent = []\n",
    "out_lab = []\n",
    "with torch.no_grad():\n",
    "    for sentence, tags in testing_data:\n",
    "        sent = prepare_sequence(sentence, word_vocab)\n",
    "        sent_tag_scores = model(sent)\n",
    "        predict = sent_tag_scores.argmax(1)\n",
    "        \n",
    "        label = prepare_sequence(tags, label_vocab)\n",
    "        \n",
    "        out_sent.append(predict.tolist())\n",
    "        out_lab.append(label.tolist())\n",
    "        \n",
    "    out_sent = [item for sublists in out_sent for item in sublists]\n",
    "    out_lab = [item for sublists in out_lab for item in sublists]\n",
    "        \n",
    "    ## print(out_sent)\n",
    "    ## print(out_lab)\n",
    "        \n",
    "    print(classification_report(out_sent, out_lab))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
